# -*- coding: utf-8 -*-
"""advisor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TZ2p1tKkoDfqHLoYW5YD0DTJUVVBUDPa
"""

import yaml
import json
from datetime import datetime
import uuid

import yaml

policy_yaml = """
version: "1.0.0"
updated_at: "2025-11-09"
domains:
  medical:
    allowed: ["explain_concepts", "summarize_guidelines", "define_terms"]
    caution: ["compare_options_without_recommendation", "general_risk_awareness"]
    escalate: ["interpret_symptoms", "medication_queries", "treatment_plans"]
    deny: ["dosage_recommendation", "diagnosis", "emergency_advice"]
    disclaimers: ["General information only. Not medical advice."]
    retrieval_sources: ["who_guidelines", "nih_faq"]
  legal:
    allowed: ["explain_concepts", "summarize_regulations", "define_terms"]
    caution: ["procedural_overviews_without_instructions", "jurisdiction_awareness"]
    escalate: ["contract_specifics", "case_law_interpretation", "form_selection"]
    deny: ["draft_contracts", "legal_strategy", "personalized_legal_recommendations"]
    disclaimers: ["General information only. Not legal advice."]
    retrieval_sources: ["gov_regulatory_faq", "official_court_summaries"]
  financial:
    allowed: ["explain_concepts", "summarize_policies", "define_terms"]
    caution: ["risk_categories_overview", "non_personalized_comparisons"]
    escalate: ["portfolio_discussion", "loan_or_tax_personalization", "investment_timing"]
    deny: ["buy_sell_recommendations", "personalized_tax_advice", "credit_or_loan_approval_guidance"]
    disclaimers: ["General information only. Not financial advice."]
    retrieval_sources: ["regulatory_faq", "central_bank_guides"]
routing_rules:
  low: "respond_grounded"
  moderate: "respond_with_caution_and_grounding"
  high: "escalate_to_human"
  critical: "deny_and_route_emergency_or_compliance"
intents:
  explain_concepts: {risk: "low"}
  summarize_guidelines: {risk: "low"}
  compare_options_without_recommendation: {risk: "moderate"}
  interpret_symptoms: {risk: "high"}
  buy_sell_recommendations: {risk: "critical"}
  ambiguous_domain: {risk: "moderate"}
"""

policy = yaml.safe_load(policy_yaml)
print(policy["intents"])

def create_event_log(user_text, domain, intent, risk, verdict, evidence=None):
    return {
        "event_version": "1.0.0",
        "timestamp": datetime.utcnow().isoformat(),
        "request_id": str(uuid.uuid4()),
        "input": {"text": user_text, "language": "en"},
        "classification": {"domain": domain, "intent": intent},
        "policy": {"risk_level": risk, "policy_version": policy["version"]},
        "retrieval": {"evidence": evidence or []},
        "verdict": {"label": verdict},
        "actions": {"responded": verdict.startswith("respond")},
        "metrics": {"latency_ms": 0}
    }

log = create_event_log(
    user_text="What is hypertension?",
    domain="medical",
    intent="explain_concepts",
    risk="low",
    verdict="respond_grounded",
    evidence=[{"source":"who_guidelines","doc_id":"who-2023-abc","score":0.78}]
)

print(json.dumps(log, indent=2))

from datetime import datetime, timezone

timestamp = datetime.now(timezone.utc).isoformat()

event_log = {
    "event_version": "1.0.0",
    "timestamp": "2025-11-09T09:53:29.138570",
    "request_id": "7929fa42-8ae6-4e29-9955-35d0f1159afc",
    "input": {
        "text": "What is hypertension?",
        "language": "en"
    },
    "classification": {
        "domain": "medical",
        "intent": "explain_concepts"
    },
    "policy": {
        "risk_level": "low",
        "policy_version": "1.0.0"
    },
    "retrieval": {
        "evidence": [
            {
                "source": "who_guidelines",
                "doc_id": "who-2023-abc",
                "score": 0.78
            }
        ]
    },
    "verdict": {
        "label": "respond_grounded"
    },
    "actions": {
        "responded": True
    },
    "metrics": {
        "latency_ms": 0
    }
}

print(event_log)

import re

# Simple keyword maps for each domain
DOMAIN_KEYWORDS = {
    "medical": ["hypertension", "diabetes", "symptom", "treatment", "disease", "medicine"],
    "legal": ["contract", "law", "court", "jurisdiction", "case", "regulation"],
    "financial": ["loan", "tax", "investment", "credit", "portfolio", "bank"],
}

INTENT_KEYWORDS = {
    "explain_concepts": ["what is", "define", "meaning of", "explain"],
    "summarize_guidelines": ["guidelines", "rules", "policy"],
    "compare_options_without_recommendation": ["compare", "difference between"],
    "interpret_symptoms": ["symptom", "feel", "pain", "diagnose"],
    "buy_sell_recommendations": ["should I buy", "sell", "invest in"],
}

def classify_query(text):
    text_lower = text.lower()
    domain, intent, confidence, rationale = "general", "ambiguous_domain", 0.0, "No strong match"

    # Domain detection
    for d, keywords in DOMAIN_KEYWORDS.items():
        if any(k in text_lower for k in keywords):
            domain = d
            confidence = 0.7
            rationale = f"Matched domain keyword(s): {', '.join([k for k in keywords if k in text_lower])}"
            break

    # Intent detection
    for i, keywords in INTENT_KEYWORDS.items():
        if any(k in text_lower for k in keywords):
            intent = i
            confidence = max(confidence, 0.8)
            rationale += f"; matched intent keyword(s): {', '.join([k for k in keywords if k in text_lower])}"
            break

    return {
        "domain": domain,
        "intent": intent,
        "confidence": confidence,
        "rationale": rationale
    }

# Example usage
classification = classify_query("What is hypertension?")
print(classification)

def generate_event(user_text):
    # Step 1: classify the query
    classification = classify_query(user_text)
    domain = classification["domain"]
    intent = classification["intent"]
    confidence = classification["confidence"]
    rationale = classification["rationale"]

    # Step 2: look up risk level from policy
    risk_level = policy["intents"].get(intent, {}).get("risk", "moderate")
    routing = policy["routing_rules"][risk_level]

    # Step 3: build event log
    event_log = {
        "event_version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "request_id": str(uuid.uuid4()),
        "input": {"text": user_text, "language": "en"},
        "classification": {
            "domain": domain,
            "intent": intent,
            "confidence": confidence,
            "rationale": rationale
        },
        "policy": {
            "risk_level": risk_level,
            "policy_version": policy["version"]
        },
        "retrieval": {"evidence": []},  # retrieval will be added in Step 3
        "verdict": {"label": routing},
        "actions": {"responded": routing.startswith("respond")},
        "metrics": {"latency_ms": 0}
    }
    return event_log

# Example run
log = generate_event("What is hypertension?")
print(json.dumps(log, indent=2))

def retrieve_evidence(domain, query, top_k=2):
    # Temporary stub until we add FAISS/HuggingFace
    if domain == "medical":
        return [
            {
                "source": "who_guidelines",
                "doc_id": "who-2023-abc",
                "score": 0.78,
                "snippet": "Hypertension is a condition where blood pressure is consistently too high."
            }
        ]
    elif domain == "legal":
        return [
            {
                "source": "gov_regulatory_faq",
                "doc_id": "law-2024-xyz",
                "score": 0.72,
                "snippet": "Contracts are legally binding agreements enforceable by law."
            }
        ]
    elif domain == "financial":
        return [
            {
                "source": "central_bank_guides",
                "doc_id": "fin-2025-ghi",
                "score": 0.75,
                "snippet": "An investment is the allocation of resources with the expectation of future returns."
            }
        ]
    else:
        return []

def generate_event(user_text):
    # Step 1: classify the query
    classification = classify_query(user_text)
    domain = classification["domain"]
    intent = classification["intent"]
    confidence = classification["confidence"]
    rationale = classification["rationale"]

    # Step 2: look up risk level from policy
    risk_level = policy["intents"].get(intent, {}).get("risk", "moderate")
    routing = policy["routing_rules"][risk_level]

    # Step 3: retrieve evidence (stub for now: only medical corpus)
    evidence = retrieve_evidence(domain, user_text, top_k=2)

    # Step 4: build event log
    event_log = {
        "event_version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "request_id": str(uuid.uuid4()),
        "input": {"text": user_text, "language": "en"},
        "classification": {
            "domain": domain,
            "intent": intent,
            "confidence": confidence,
            "rationale": rationale
        },
        "policy": {
            "risk_level": risk_level,
            "policy_version": policy["version"]
        },
        "retrieval": {"evidence": evidence},
        "verdict": {"label": routing},
        "actions": {"responded": routing.startswith("respond")},
        "metrics": {"latency_ms": 0}
    }
    return event_log

# Example run
log = generate_event("What is hypertension?")
print(json.dumps(log, indent=2))

def decide_verdict(risk_level, evidence_scores):
    max_score = max(evidence_scores) if evidence_scores else 0.0

    if risk_level == "low":
        if max_score >= 0.6:
            return "respond_grounded"
        else:
            return "respond_with_caution_and_grounding"

    elif risk_level == "moderate":
        if max_score >= 0.6:
            return "respond_with_caution_and_grounding"
        else:
            return "escalate_to_human"

    elif risk_level == "high":
        return "escalate_to_human"

    elif risk_level == "critical":
        return "deny_and_route_emergency_or_compliance"

    return "respond_with_caution_and_grounding"  # fallback

def generate_event(user_text):
    classification = classify_query(user_text)
    domain = classification["domain"]
    intent = classification["intent"]

    risk_level = policy["intents"].get(intent, {}).get("risk", "moderate")
    evidence = retrieve_evidence(domain, user_text, top_k=2)
    evidence_scores = [e["score"] for e in evidence]

    verdict_label = decide_verdict(risk_level, evidence_scores)

    event_log = {
        "event_version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "request_id": str(uuid.uuid4()),
        "input": {"text": user_text, "language": "en"},
        "classification": classification,
        "policy": {"risk_level": risk_level, "policy_version": policy["version"]},
        "retrieval": {"evidence": evidence},
        "verdict": {"label": verdict_label},
        "actions": {"responded": verdict_label.startswith("respond")},
        "metrics": {"latency_ms": 0}
    }
    return event_log

# Test queries across domains
queries = [
    "What is hypertension?",                  # medical, low risk, strong evidence
    "Explain contract law basics",            # legal, low risk, stub evidence
    "Should I buy Tesla stock?",              # financial, critical risk
    "Compare diabetes and hypertension",      # medical, moderate risk
    "Draft a contract for employment",        # legal, critical risk
]

for q in queries:
    log = generate_event(q)
    print(f"\nQuery: {q}")
    print(json.dumps(log, indent=2))

!pip install faiss-cpu sentence-transformers

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

embedder = SentenceTransformer("all-MiniLM-L6-v2")

medical_docs = [
    {"doc_id": "who-2023-abc", "text": "Hypertension is a condition where blood pressure is consistently too high."},
    {"doc_id": "nih-2022-def", "text": "Treatment of hypertension involves lifestyle changes and sometimes medication."}
]

legal_docs = [
    {"doc_id": "law-2024-xyz", "text": "Contracts are legally binding agreements enforceable by law."},
    {"doc_id": "law-2025-klm", "text": "Employment contracts outline rights and obligations of both employer and employee."}
]

financial_docs = [
    {"doc_id": "fin-2025-ghi", "text": "An investment is the allocation of resources with the expectation of future returns."},
    {"doc_id": "fin-2025-jkl", "text": "Stock trading involves buying and selling shares in public companies."}
]

def build_index(docs):
    texts = [d["text"] for d in docs]
    embeddings = embedder.encode(texts)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, docs

medical_index, medical_docs = build_index(medical_docs)
legal_index, legal_docs = build_index(legal_docs)
financial_index, financial_docs = build_index(financial_docs)

def retrieve_evidence(domain, query, top_k=2):
    query_vec = embedder.encode([query])
    if domain == "medical":
        scores, idxs = medical_index.search(np.array(query_vec), top_k)
        docs = medical_docs
    elif domain == "legal":
        scores, idxs = legal_index.search(np.array(query_vec), top_k)
        docs = legal_docs
    elif domain == "financial":
        scores, idxs = financial_index.search(np.array(query_vec), top_k)
        docs = financial_docs
    else:
        return []

    results = []
    for score, idx in zip(scores[0], idxs[0]):
        doc = docs[idx]
        results.append({
            "source": domain + "_corpus",
            "doc_id": doc["doc_id"],
            "score": float(score),
            "snippet": doc["text"]
        })
    return results

log = generate_event("Explain contract law basics")
print(json.dumps(log, indent=2))

log = generate_event("What is hypertension?")
print(json.dumps(log, indent=2))

!pip install faiss-cpu sentence-transformers beautifulsoup4 requests PyPDF2

import requests
from bs4 import BeautifulSoup

# Fetch WHO hypertension fact sheet
url_medical = "https://www.who.int/news-room/fact-sheets/detail/hypertension"
response = requests.get(url_medical)
soup = BeautifulSoup(response.text, "html.parser")
medical_text = soup.get_text()

def chunk_text(text, size=200):
    words = text.split()
    return [" ".join(words[i:i+size]) for i in range(0, len(words), size)]

medical_chunks = chunk_text(medical_text)

import PyPDF2

url_legal = "https://www.indiacode.nic.in/bitstream/123456789/2187/2/A187209.pdf"
response = requests.get(url_legal)
with open("contract_act.pdf", "wb") as f:
    f.write(response.content)

reader = PyPDF2.PdfReader("contract_act.pdf")
legal_text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
legal_chunks = chunk_text(legal_text)

url_financial = "https://investor.sebi.gov.in/iematerial.html"
response = requests.get(url_financial)
soup = BeautifulSoup(response.text, "html.parser")
financial_text = soup.get_text()
financial_chunks = chunk_text(financial_text)

from sentence_transformers import SentenceTransformer
import faiss, numpy as np

embedder = SentenceTransformer("all-MiniLM-L6-v2")

def build_index(chunks):
    embeddings = embedder.encode(chunks)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, chunks

medical_index, medical_chunks = build_index(medical_chunks)
legal_index, legal_chunks = build_index(legal_chunks)
financial_index, financial_chunks = build_index(financial_chunks)

def retrieve_evidence(domain, query, top_k=3):
    q_vec = embedder.encode([query])
    if domain == "medical":
        scores, idxs = medical_index.search(np.array(q_vec), top_k)
        chunks = medical_chunks
    elif domain == "legal":
        scores, idxs = legal_index.search(np.array(q_vec), top_k)
        chunks = legal_chunks
    elif domain == "financial":
        scores, idxs = financial_index.search(np.array(q_vec), top_k)
        chunks = financial_chunks
    else:
        return []

    return [
        {"source": domain+"_corpus", "doc_id": f"chunk-{i}", "score": float(s), "snippet": chunks[i]}
        for s, i in zip(scores[0], idxs[0])
    ]

print(retrieve_evidence("medical", "What is hypertension?", top_k=2))
print(retrieve_evidence("legal", "Explain contract law basics", top_k=2))
print(retrieve_evidence("financial", "What are mutual funds?", top_k=2))

import requests
from bs4 import BeautifulSoup

def fetch_clean(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    return soup.get_text()

cdc_text = fetch_clean("https://www.cdc.gov/bloodpressure/about.htm")
nih_text = fetch_clean("https://www.nhlbi.nih.gov/health/high-blood-pressure")
rbi_text = fetch_clean("https://www.rbi.org.in/financialeducation/home.aspx")
legfaq_text = fetch_clean("https://legislative.gov.in/faq-page")

import PyPDF2

url_contract = "https://www.indiacode.nic.in/bitstream/123456789/2187/2/A187209.pdf"
response = requests.get(url_contract)
with open("contract_act.pdf", "wb") as f:
    f.write(response.content)

reader = PyPDF2.PdfReader("contract_act.pdf")
contract_text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])

def chunk_text(text, size=200):
    words = text.split()
    return [" ".join(words[i:i+size]) for i in range(0, len(words), size)]

cdc_chunks = chunk_text(cdc_text)
nih_chunks = chunk_text(nih_text)
rbi_chunks = chunk_text(rbi_text)
legfaq_chunks = chunk_text(legfaq_text)
contract_chunks = chunk_text(contract_text)

from sentence_transformers import SentenceTransformer
import faiss, numpy as np

embedder = SentenceTransformer("all-MiniLM-L6-v2")

def build_index(chunks):
    embeddings = embedder.encode(chunks)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, chunks

cdc_index, cdc_chunks = build_index(cdc_chunks)
nih_index, nih_chunks = build_index(nih_chunks)
rbi_index, rbi_chunks = build_index(rbi_chunks)
legfaq_index, legfaq_chunks = build_index(legfaq_chunks)
contract_index, contract_chunks = build_index(contract_chunks)

def retrieve_evidence(domain, query, top_k=3):
    q_vec = embedder.encode([query])
    if domain == "medical":
        scores, idxs = cdc_index.search(np.array(q_vec), top_k)
        chunks = cdc_chunks
    elif domain == "legal":
        scores, idxs = contract_index.search(np.array(q_vec), top_k)
        chunks = contract_chunks
    elif domain == "financial":
        scores, idxs = rbi_index.search(np.array(q_vec), top_k)
        chunks = rbi_chunks
    else:
        return []

    return [
        {"source": domain+"_corpus", "doc_id": f"chunk-{i}", "score": float(s), "snippet": chunks[i]}
        for s, i in zip(scores[0], idxs[0])
    ]

user_text = "What is hypertension?"
domain = "medical"   # can be "medical", "legal", or "financial"


evidence = retrieve_evidence(domain, user_text, top_k=2)
print(evidence)

main_content = soup.find("main")
medical_text = main_content.get_text() if main_content else soup.get_text()

chunks = []
clean_chunks = [c for c in chunks if "Contact Us" not in c and "Page Not Found" not in c]

import requests
from bs4 import BeautifulSoup
import PyPDF2
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

# 1. Fetch and clean HTML
def fetch_clean(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    main_content = soup.find("main") or soup.find("article") or soup
    text = main_content.get_text()
    return text

# Example: CDC hypertension page
cdc_text = fetch_clean("https://www.cdc.gov/bloodpressure/about.htm")

# 2. Chunk text
def chunk_text(text, size=200):
    words = text.split()
    return [" ".join(words[i:i+size]) for i in range(0, len(words), size)]

cdc_chunks = chunk_text(cdc_text)

# 3. Filter out boilerplate
clean_chunks = [
    c for c in cdc_chunks
    if "Contact Us" not in c and "Page Not Found" not in c and "CDC Archive" not in c
]

# 4. Build FAISS index
embedder = SentenceTransformer("all-MiniLM-L6-v2")

def build_index(chunks):
    embeddings = embedder.encode(chunks)
    dim = embeddings.shape[1]
    index = fa

import requests
from bs4 import BeautifulSoup
import PyPDF2
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# --- 1. Fetch + Clean ---
def fetch_clean(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    main_content = soup.find("main") or soup.find("article") or soup
    text = main_content.get_text()
    return text

# WHO
who_text = fetch_clean("https://www.who.int/news-room/fact-sheets/detail/hypertension")

# CDC
cdc_text = fetch_clean("https://www.cdc.gov/bloodpressure/about.htm")

# NIH
nih_text = fetch_clean("https://www.nhlbi.nih.gov/health/high-blood-pressure")

# RBI
rbi_text = fetch_clean("https://www.rbi.org.in/financialeducation/home.aspx")

# SEBI
sebi_text = fetch_clean("https://investor.sebi.gov.in/iematerial.html")

# Legislative FAQs
legfaq_text = fetch_clean("https://legislative.gov.in/faq-page")

# India Code PDF
url_contract = "https://www.indiacode.nic.in/bitstream/123456789/2187/2/A187209.pdf"
response = requests.get(url_contract)
with open("contract_act.pdf", "wb") as f:
    f.write(response.content)
reader = PyPDF2.PdfReader("contract_act.pdf")
contract_text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])

# --- 2. Chunk ---
def chunk_text(text, size=200):
    words = text.split()
    return [" ".join(words[i:i+size]) for i in range(0, len(words), size)]

who_chunks = chunk_text(who_text)
cdc_chunks = chunk_text(cdc_text)
nih_chunks = chunk_text(nih_text)
rbi_chunks = chunk_text(rbi_text)
sebi_chunks = chunk_text(sebi_text)
legfaq_chunks = chunk_text(legfaq_text)
contract_chunks = chunk_text(contract_text)

# --- 3. Filter boilerplate (example for CDC) ---
cdc_chunks = [c for c in cdc_chunks if "Contact Us" not in c and "Page Not Found" not in c]

# --- 4. Build FAISS indices ---
def build_index(chunks):
    embeddings = embedder.encode(chunks)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, chunks

who_index, who_chunks = build_index(who_chunks)
cdc_index, cdc_chunks = build_index(cdc_chunks)
nih_index, nih_chunks = build_index(nih_chunks)
rbi_index, rbi_chunks = build_index(rbi_chunks)
sebi_index, sebi_chunks = build_index(sebi_chunks)
legfaq_index, legfaq_chunks = build_index(legfaq_chunks)
contract_index, contract_chunks = build_index(contract_chunks)

import json
from datetime import datetime, timezone
import uuid

# --- Improved Classification ---
def classify_query(user_text):
    text = user_text.lower()
    if any(k in text for k in ["hypertension", "blood pressure", "diabetes", "cancer"]):
        return {"domain": "medical", "intent": "explain_concepts", "confidence": 0.8,
                "rationale": "Matched medical keyword(s)"}
    elif any(k in text for k in ["contract", "law", "agreement", "legal"]):
        return {"domain": "legal", "intent": "explain_concepts", "confidence": 0.8,
                "rationale": "Matched legal keyword(s)"}
    elif any(k in text for k in ["mutual fund", "funds", "investment", "stock", "etf", "share", "bond"]):
        return {"domain": "financial", "intent": "explain_concepts", "confidence": 0.8,
                "rationale": "Matched financial keyword(s)"}
    else:
        return {"domain": "general", "intent": "ambiguous_domain", "confidence": 0.0,
                "rationale": "No strong match"}

# --- Threshold-Aware Verdict Logic ---
def decide_verdict(risk_level, evidence_scores, threshold=1.0):
    if risk_level == "critical":
        return "escalate_or_deny"
    if not evidence_scores:
        return "escalate_to_human"

    best_score = min(evidence_scores)  # lower distance = better match
    if best_score < threshold and risk_level == "low":
        return "respond_grounded"
    elif best_score < threshold and risk_level == "moderate":
        return "respond_with_caution_and_grounding"
    else:
        return "escalate_to_human"

# --- Generate Event ---
def generate_event(user_text):
    classification = classify_query(user_text)
    domain = classification["domain"]
    intent = classification["intent"]

    # Policy lookup
    risk_level = policy["intents"].get(intent, {}).get("risk", "moderate")
    disclaimers = policy["domains"].get(domain, {}).get("disclaimers", [])

    # Retrieval
    evidence = retrieve_evidence(domain, user_text, top_k=3)
    evidence_scores = [e["score"] for e in evidence]

    # Verdict
    verdict_label = decide_verdict(risk_level, evidence_scores)

    # Event log
    event_log = {
        "event_version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "request_id": str(uuid.uuid4()),
        "input": {"text": user_text, "language": "en"},
        "classification": classification,
        "policy": {"risk_level": risk_level, "policy_version": policy["version"]},
        "retrieval": {"evidence": evidence},
        "verdict": {"label": verdict_label},
        "actions": {
            "responded": verdict_label.startswith("respond"),
            "disclaimers": disclaimers
        },
        "metrics": {"latency_ms": 0}
    }

    return json.dumps(event_log, indent=2)

# --- Re-fetch and clean corpora ---
cdc_text = fetch_clean("https://www.cdc.gov/bloodpressure/about.htm")
nih_text = fetch_clean("https://www.nhlbi.nih.gov/health/high-blood-pressure")
sebi_text = fetch_clean("https://investor.sebi.gov.in/iematerial.html")
rbi_text = fetch_clean("https://www.rbi.org.in/financialeducation/home.aspx")

# --- Chunk ---
cdc_chunks = chunk_text(cdc_text)
nih_chunks = chunk_text(nih_text)
sebi_chunks = chunk_text(sebi_text)
rbi_chunks = chunk_text(rbi_text)

# --- Filter boilerplate (CDC example) ---
cdc_chunks = [c for c in cdc_chunks if "Page Not Found" not in c and "Contact Us" not in c]

# --- Rebuild FAISS indices ---
cdc_index, cdc_chunks = build_index(cdc_chunks)
nih_index, nih_chunks = build_index(nih_chunks)
sebi_index, sebi_chunks = build_index(sebi_chunks)
rbi_index, rbi_chunks = build_index(rbi_chunks)

def fetch_clean(url, boilerplate_phrases=None):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    main_content = soup.find("main") or soup.find("article") or soup.find("body")
    text = main_content.get_text(separator=" ", strip=True)

    if boilerplate_phrases:
        for phrase in boilerplate_phrases:
            text = text.replace(phrase, "")

    return " ".join(text.split())

def chunk_and_filter(text, size=200, drop_phrases=None):
    words = text.split()
    chunks = [" ".join(words[i:i+size]) for i in range(0, len(words), size)]
    if drop_phrases:
        chunks = [c for c in chunks if all(p not in c for p in drop_phrases)]
    return chunks

def build_index(chunks, embedder):
    embeddings = embedder.encode(chunks)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, chunks

sources = {
    "who": {"url": "https://www.who.int/news-room/fact-sheets/detail/hypertension"},
    "cdc": {"url": "https://www.cdc.gov/bloodpressure/about.htm",
            "boilerplate": ["Page Not Found", "Contact Us", "CDC Archive"]},
    "nih": {"url": "https://www.nhlbi.nih.gov/health/high-blood-pressure"},
    "sebi": {"url": "https://investor.sebi.gov.in/iematerial.html"},
    "rbi": {"url": "https://www.rbi.org.in/financialeducation/home.aspx"},
    "legfaq": {"url": "https://legislative.gov.in/faq-page"},
}

embedder = SentenceTransformer("all-MiniLM-L6-v2")
indices = {}

for name, cfg in sources.items():
    text = fetch_clean(cfg["url"], cfg.get("boilerplate"))
    chunks = chunk_and_filter(text, drop_phrases=cfg.get("boilerplate"))
    index, chunks = build_index(chunks, embedder)
    indices[name] = {"index": index, "chunks": chunks}

# --- Contract Act Corpus Ingestion ---
url_contract = "https://www.indiacode.nic.in/bitstream/123456789/2187/2/A187209.pdf"
response = requests.get(url_contract)
with open("contract_act.pdf", "wb") as f:
    f.write(response.content)

reader = PyPDF2.PdfReader("contract_act.pdf")
contract_text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])

contract_chunks = chunk_and_filter(contract_text, size=200)
contract_index, contract_chunks = build_index(contract_chunks, embedder)

# Add to indices dict
indices["contract"] = {"index": contract_index, "chunks": contract_chunks}

def retrieve_evidence(domain, query, top_k=3):
    q_vec = embedder.encode([query])
    results = []

    if domain == "medical":
        sources = ["who", "cdc", "nih"]
    elif domain == "legal":
        sources = ["legfaq", "contract"]
    elif domain == "financial":
        sources = ["sebi", "rbi"]
    else:
        return []

    for src in sources:
        index = indices[src]["index"]
        chunks = indices[src]["chunks"]
        scores, idxs = index.search(np.array(q_vec), top_k)
        for s, i in zip(scores[0], idxs[0]):
            results.append({
                "source": f"{domain}_{src}",
                "doc_id": f"chunk-{i}",
                "score": float(s),
                "snippet": chunks[i]
            })

    return sorted(results, key=lambda x: x["score"])[:top_k]

def generate_event(user_text):
    classification = classify_query(user_text)
    domain = classification["domain"]
    intent = classification["intent"]

    risk_level = policy["intents"].get(intent, {}).get("risk", "moderate")
    disclaimers = policy["domains"].get(domain, {}).get("disclaimers", [])

    evidence = retrieve_evidence(domain, user_text, top_k=3)
    evidence_scores = [e["score"] for e in evidence]

    verdict_label = decide_verdict(risk_level, evidence_scores)

    event_log = {
        "event_version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "request_id": str(uuid.uuid4()),
        "input": {"text": user_text, "language": "en"},
        "classification": classification,
        "policy": {"risk_level": risk_level, "policy_version": policy["version"]},
        "retrieval": {"evidence": evidence},
        "verdict": {"label": verdict_label},
        "actions": {
            "responded": verdict_label.startswith("respond"),
            "disclaimers": disclaimers
        },
        "metrics": {"latency_ms": 0}
    }

    return json.dumps(event_log, indent=2)

# Define test queries across domains
test_queries = [
    "What is hypertension?",        # Medical
    "Explain contract law basics",  # Legal
    "What are mutual funds?"        # Financial
]

# Run simulation and print event logs
for q in test_queries:
    log_json = generate_event(q)
    print("\n=== Event Log for Query:", q, "===\n")
    print(log_json)

with open("event_log.json", "a") as f:
    for q in ["What is hypertension?", "Explain contract law basics", "What are mutual funds?"]:
        f.write(generate_event(q) + "\n")